kind: Template
apiVersion: v1
metadata:
  annotations:
    description: HDFS-Spark template for OpenShift Container Platform. It includes HDFS, Spark, Zeppelin and Jupyter apps.
    iconClass: "fa fa-star-o"
  name: spark
objects:
- kind: ServiceAccount
  apiVersion: v1
  metadata:
    name: hadoop
- kind: ImageStream
  apiVersion: v1
  metadata:
    labels:
      app: ${SPARK_MASTER}
    name: ${SPARK_MASTER}
  spec:
    tags:
    - name: latest
      from:
        kind: DockerImage
        name: radanalyticsio/openshift-spark
      referencePolicy:
        type: Source
- kind: ImageStream
  apiVersion: v1
  metadata:
    labels:
      app: ${ZEPPELIN}
    name: ${ZEPPELIN}
  spec:
    tags:
    - name: latest
      from:
        kind: DockerImage
        name: gcr.io/google_containers/zeppelin:v0.5.6_v1
      referencePolicy:
        type: Source
- kind: ImageStream
  apiVersion: v1
  metadata:
    labels:
      app: ${JUPYTER}
    name: ${JUPYTER}
  spec:
    tags:
    - name: latest
      from:
        kind: DockerImage
        name: tmckay/pyspark-hdfs-notebook:latest
      referencePolicy:
        type: Source        
#HDFS-namenode section
- kind: StatefulSet
  apiVersion: apps/v1beta1
  metadata:
    name: ${HDFS}-namenode
  spec:
    serviceName: "${HDFS}-namenode"
    replicas: ${HDFS_NAMENODES}
    template:
      metadata:
        labels:
          app: ${HDFS}-namenode
      spec:
        terminationGracePeriodSeconds: 0
        containers:
          - name: ${HDFS}-namenode
            image: agallego/namenode:latest
            env:
              - name: CLUSTER_NAME
                value: hdfs-k8s
              - name: CORE_CONF_fs_defaultFS
                value: hdfs://${HDFS}-namenode-0.hdfs-namenode.${NAMESPACE}.svc.cluster.local:8020
              - name: HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check
                value: "false"
              - name: HDFS_CONF_dfs_permissions_enabled
                value: "false"
              - name: HDFS_CONF_dfs_webhdfs_enabled
                value: "true"                                  
            ports:
            - containerPort: 8000
              name: hfx            
            - containerPort: 8020
              name: fs
            - containerPort: 50070
              name: namenode-web
            volumeMounts:
            - name: hadoop-data
              mountPath: /hadoop/dfs/data
        restartPolicy: Always
        serviceAccount: hadoop
    volumeClaimTemplates:
    - metadata:
        name: hadoop-data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: ${HDFS_NAMENODE_VOLSIZE}
- kind: Service
  apiVersion: v1
  metadata:
    name: ${HDFS}-namenode
    labels:
      app: ${HDFS}-namenode
  spec:
    ports:
    - port: 8000
      name: hfx
    - port: 8020
      name: fs
    - port: 50070
      name: namenode-web
    clusterIP: None
    selector:
      app: ${HDFS}-namenode
- apiVersion: v1
  kind: Route
  metadata:
    name: ${HDFS}-namenode
  spec:
    host: 
    port:
      targetPort: 50070
    to:
      kind: Service
      name: ${HDFS}-namenode
      weight: 100
    wildcardPolicy: None
- apiVersion: v1
  kind: Route
  metadata:
    name: ${HDFS}-namenode-hfx
  spec:
    host: ${HDFS}-namenode-hfx
    port:
      targetPort: 8000
    to:
      kind: Service
      name: ${HDFS}-namenode
      weight: 100
    wildcardPolicy: None    
#HDFS-datanode section
- kind: StatefulSet
  apiVersion: apps/v1beta1
  metadata:
    name: ${HDFS}-datanode
  spec:
    serviceName: "${HDFS}-datanode"
    replicas: ${HDFS_DATANODES}
    template:
      metadata:
        labels:
          app: ${HDFS}-datanode
      spec:
        containers:
          - name: datanode
            image: uhopper/hadoop-datanode:2.7.2
            env:
              - name: CORE_CONF_fs_defaultFS
                value: hdfs://${HDFS}-namenode:8020
              - name: HDFS_CONF_dfs_webhdfs_enabled
                value: "true"                  
            ports:
            - containerPort: 50010
              name: fs
            volumeMounts:
            - name: hadoop-data
              mountPath: /hadoop/dfs/data
        restartPolicy: Always
        serviceAccount: hadoop
    volumeClaimTemplates:
    - metadata:
        name: hadoop-data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: ${HDFS_DATANODE_VOLSIZE}
- kind: Service
  apiVersion: v1
  metadata:
    name: ${HDFS}-datanode
    labels:
      app: ${HDFS}-datanode
  spec:
    ports:
    - port: 50010
      name: fs
    clusterIP: None
    selector:
      app: ${HDFS}-datanode            
#SPARK section
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    labels:
      oshinko-cluster: ${SPARK_MASTER}
      oshinko-type: master
    name: ${SPARK_MASTER}
  spec:
    replicas: 1
    selector:
      oshinko-cluster: ${SPARK_MASTER}
    strategy:
      activeDeadlineSeconds: 21600
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        labels:
          oshinko-cluster: ${SPARK_MASTER}
          oshinko-type: master
      spec:
        containers:
        - name: ${SPARK_MASTER}
          env:
          - name: OSHINKO_SPARK_CLUSTER
            value: ${SPARK_MASTER}
          - name: OSHINKO_REST_HOST
          - name: OSHINKO_REST_PORT            
          imagePullPolicy: Always
          image: ${SPARK_MASTER}:latest
          ports:
            - containerPort: "7077"
              name: spark-master
              protocol: TCP
            - containerPort: "8080"
              name: spark-webui
              protocol: TCP
          livenessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 120
              timeoutSeconds: 5             
          terminationMessagePath: /dev/termination-log
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        serviceAccount: hadoop        
        securityContext: {}
        terminationGracePeriodSeconds: 30              
    triggers:
    - type: ConfigChange
    - imageChangeParams:
        automatic: true
        containerNames:
        - ${SPARK_MASTER}
        from:
          kind: ImageStreamTag
          name: ${SPARK_MASTER}:latest
      type: ImageChange
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      oshinko-cluster: ${SPARK_MASTER}
      oshinko-type: master
    name: ${SPARK_MASTER}
  spec:
    ports:
    - port: 7077
      protocol: TCP
      targetPort: 7077
    selector:
      oshinko-type: master
    sessionAffinity: None
    type: ClusterIP
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      oshinko-cluster: ${SPARK_MASTER}
      oshinko-type: webui
    name: ${SPARK_MASTER}-ui
  spec:
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      oshinko-type: master
    sessionAffinity: None
    type: ClusterIP
- apiVersion: v1
  kind: Route
  metadata:
    name: ${SPARK_MASTER}-ui-route
    oshinko-cluster: ${SPARK_MASTER}
    oshinko-type: webui    
  spec:
    host: 
    to:
      kind: Service
      name: ${SPARK_MASTER}-ui
      weight: 100
    wildcardPolicy: None
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    labels:
      oshinko-cluster: ${SPARK_MASTER}
      oshinko-type: worker
    name: ${SPARK_WORKER}
  spec:
    replicas: 1
    selector:
      oshinko-cluster: ${SPARK_MASTER}  
    strategy:
      activeDeadlineSeconds: 21600
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        labels:
          oshinko-cluster: ${SPARK_MASTER}
          oshinko-type: worker
      spec:
        containers:
        - name: ${SPARK_WORKER}
          env:
            - name: OSHINKO_SPARK_CLUSTER
              value: ${SPARK_MASTER}
            - name: OSHINKO_REST_HOST
            - name: OSHINKO_REST_PORT              
            - name: SPARK_MASTER_ADDRESS
              value: spark://${SPARK_MASTER}:7077
            - name: SPARK_MASTER_UI_ADDRESS
              value: http://${SPARK_MASTER}-ui:8080
          imagePullPolicy: Always
          image: ${SPARK_MASTER}:latest
          ports:
            - containerPort: 8081
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 8081
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        serviceAccount: hadoop        
        securityContext: {}
        terminationGracePeriodSeconds: 30              
    triggers:
    - type: ConfigChange
    - imageChangeParams:
        automatic: true
        containerNames:
        - ${SPARK_WORKER}
        from:
          kind: ImageStreamTag
          name: ${SPARK_MASTER}:latest
      type: ImageChange
- apiVersion: autoscaling/v1
  kind: HorizontalPodAutoscaler
  metadata:
    creationTimestamp: null
    labels:
      oshinko-cluster: ${SPARK_MASTER}
      oshinko-type: worker
    name: ${SPARK_WORKER}
  spec:
    maxReplicas: ${SPARK_WORKER_REPLICAS}
    minReplicas: 1
    scaleTargetRef:
      apiVersion: extensions/v1beta1
      kind: DeploymentConfig
      name: ${SPARK_WORKER}
    targetCPUUtilizationPercentage: ${SPARK_WORKER_REPLICAS_TRIGGER}
#ZEPPELIN section
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    labels:
      app: ${ZEPPELIN}
    name: ${ZEPPELIN}
  spec:
    replicas: 1
    selector:
      app: ${ZEPPELIN}
      deploymentconfig: ${ZEPPELIN}    
    strategy:
      activeDeadlineSeconds: 21600
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        labels:
          app: ${ZEPPELIN}
          deploymentconfig: ${ZEPPELIN}
      spec:
        containers:
        - name: ${ZEPPELIN}
          imagePullPolicy: Always
          image: ${ZEPPELIN}:latest
          ports:
            - containerPort: 8080
              protocol: TCP
          resources:
            requests:
              cpu: 100m
          terminationMessagePath: /dev/termination-log
          livenessProbe:
              httpGet:
                path: /
                port: 8080
              initialDelaySeconds: 120
              timeoutSeconds: 5             
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        serviceAccount: hadoop
        securityContext: {}
        terminationGracePeriodSeconds: 30              
    triggers:
    - type: ConfigChange
    - imageChangeParams:
        automatic: true
        containerNames:
        - ${ZEPPELIN}
        from:
          kind: ImageStreamTag
          name: ${ZEPPELIN}:latest
      type: ImageChange
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: ${ZEPPELIN}
    name: ${ZEPPELIN}
  spec:
    ports:
    - name: ${ZEPPELIN}
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      app: ${ZEPPELIN}
      deploymentconfig: ${ZEPPELIN}
    sessionAffinity: None
    type: ClusterIP
- apiVersion: v1
  kind: Route
  metadata:
    name: ${ZEPPELIN}
  spec:
    host: 
    port:
      targetPort: ${ZEPPELIN}
    to:
      kind: Service
      name: ${ZEPPELIN}
      weight: 100
    wildcardPolicy: None
#JUPYTER section
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    labels:
      app: ${JUPYTER}
    name: ${JUPYTER}
  spec:
    replicas: 1
    selector:
      app: ${JUPYTER}
      deploymentconfig: ${JUPYTER}    
    strategy:
      activeDeadlineSeconds: 21600
      rollingParams:
        intervalSeconds: 1
        maxSurge: 25%
        maxUnavailable: 25%
        timeoutSeconds: 600
        updatePeriodSeconds: 1
      type: Rolling
    template:
      metadata:
        labels:
          app: ${JUPYTER}
          deploymentconfig: ${JUPYTER}
      spec:
        containers:
        - name: ${JUPYTER}
          imagePullPolicy: Always
          image: ${JUPYTER}:latest
          ports:
            - containerPort: 8888
              protocol: TCP
          terminationMessagePath: /dev/termination-log
          livenessProbe:
              httpGet:
                path: /
                port: 8888
              initialDelaySeconds: 120
              timeoutSeconds: 5             
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        serviceAccount: hadoop
        securityContext: {}
        terminationGracePeriodSeconds: 30              
    triggers:
    - type: ConfigChange
    - imageChangeParams:
        automatic: true
        containerNames:
        - ${JUPYTER}
        from:
          kind: ImageStreamTag
          name: ${JUPYTER}:latest
      type: ImageChange
- apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: ${JUPYTER}
    name: ${JUPYTER}
  spec:
    ports:
    - name: ${JUPYTER}
      port: 80
      protocol: TCP
      targetPort: 8888
    selector:
      app: ${JUPYTER}
      deploymentconfig: ${JUPYTER}
    sessionAffinity: None
    type: ClusterIP
- apiVersion: v1
  kind: Route
  metadata:
    name: ${JUPYTER}
  spec:
    host: 
    port:
      targetPort: ${JUPYTER}
    to:
      kind: Service
      name: ${JUPYTER}
      weight: 100
    wildcardPolicy: None                          
parameters:
- description: Name of this project.
  name: NAMESPACE
  required: true
  value: hadoop
- description: Name for Spark master.
  name: SPARK_MASTER
  required: true
  value: sparkmaster
- description: Name for Spark Worker.
  name: SPARK_WORKER
  required: true
  value: sparkworker
- description: Maximum number of Spark Workers.
  name: SPARK_WORKER_REPLICAS
  required: true
  value: "10"
- description: Autoscaler CPU trigger for Spark Workers.
  name: SPARK_WORKER_REPLICAS_TRIGGER
  required: true
  value: "80"   
- description: Name for Zeppelin.
  name: ZEPPELIN
  required: true
  value: zeppelin
- description: Name for Jupyter.
  name: JUPYTER
  required: true
  value: jupyter  
- description: Name for HDFS.
  name: HDFS
  required: true
  value: hdfs
- description: Number of HDFS Name Nodes.
  name: HDFS_NAMENODES
  required: true
  value: "1" 
- description: Size of Name node volume.
  name: HDFS_NAMENODE_VOLSIZE
  required: true
  value: "10Gi" 
- description: Number of HDFS Data Nodes.
  name: HDFS_DATANODES
  required: true
  value: "1"
- description: Size of Data node volume.
  name: HDFS_DATANODE_VOLSIZE
  required: true
  value: "10Gi" 
